groups:
  - name: prometheus-monitoring
    rules:
      # Prometheus target down
      - alert: PrometheusTargetDown
        expr: up == 0
        for: 30s
        labels:
          severity: critical
          service: prometheus
          category: availability
        annotations:
          summary: "Prometheus target down (instance {{ $labels.instance }})"
          description: "Prometheus target {{ $labels.job }} on {{ $labels.instance }} has been down for more than 30 seconds"
          runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsdown"

      # Prometheus configuration reload failure
      - alert: PrometheusConfigurationReloadFailure
        expr: prometheus_config_last_reload_successful != 1
        for: 0m
        labels:
          severity: critical
          service: prometheus
          category: configuration
        annotations:
          summary: "Prometheus configuration reload failure"
          description: "Prometheus configuration reload failed on {{ $labels.instance }}"
          runbook_url: "https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusconfigreloadfailed"

      # Prometheus rule evaluation failures
      - alert: PrometheusRuleEvaluationFailures
        expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
        for: 0m
        labels:
          severity: critical
          service: prometheus
          category: rules
        annotations:
          summary: "Prometheus rule evaluation failures"
          description: "Prometheus has {{ $value }} rule evaluation failures in the last 5 minutes on {{ $labels.instance }}"

      # Prometheus too many restarts
      - alert: PrometheusTooManyRestarts
        expr: changes(process_start_time_seconds{job=~"prometheus|alertmanager"}[15m]) > 2
        for: 0m
        labels:
          severity: warning
          service: prometheus
          category: stability
        annotations:
          summary: "Prometheus service restarting frequently"
          description: "Prometheus service {{ $labels.job }} has restarted more than twice in the last 15 minutes on {{ $labels.instance }}"

      # Prometheus TSDB compaction failures
      - alert: PrometheusTSDBCompactionsFailing
        expr: increase(prometheus_tsdb_compactions_failed_total[3h]) > 0
        for: 0m
        labels:
          severity: warning
          service: prometheus
          category: storage
        annotations:
          summary: "Prometheus TSDB compactions failing"
          description: "Prometheus has {{ $value }} TSDB compaction failures in the last 3 hours on {{ $labels.instance }}"

      # Prometheus TSDB reloads failing
      - alert: PrometheusTSDBReloadsFailing
        expr: increase(prometheus_tsdb_reloads_failures_total[3h]) > 0
        for: 0m
        labels:
          severity: warning
          service: prometheus
          category: storage
        annotations:
          summary: "Prometheus TSDB reloads failing"
          description: "Prometheus has {{ $value }} TSDB reload failures in the last 3 hours on {{ $labels.instance }}"

  - name: alertmanager-monitoring
    rules:
      # Alertmanager down
      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 1m
        labels:
          severity: critical
          service: alertmanager
          category: availability
        annotations:
          summary: "Alertmanager is down"
          description: "Alertmanager has been down for more than 1 minute on {{ $labels.instance }}"

      # Alertmanager configuration reload failure
      - alert: AlertmanagerConfigurationReloadFailure
        expr: alertmanager_config_last_reload_successful != 1
        for: 0m
        labels:
          severity: critical
          service: alertmanager
          category: configuration
        annotations:
          summary: "Alertmanager configuration reload failure"
          description: "Alertmanager configuration reload failed on {{ $labels.instance }}"

      # Alertmanager config not synced
      - alert: AlertmanagerConfigNotSynced
        expr: count(count_values("config_hash", alertmanager_config_hash)) > 1
        for: 0m
        labels:
          severity: warning
          service: alertmanager
          category: configuration
        annotations:
          summary: "Alertmanager configurations out of sync"
          description: "Alertmanager cluster instances have different configurations"

      # Alertmanager failed to send alerts
      - alert: AlertmanagerFailedToSendAlerts
        expr: increase(alertmanager_notifications_failed_total[5m]) > 0
        for: 1m
        labels:
          severity: warning
          service: alertmanager
          category: notifications
        annotations:
          summary: "Alertmanager failed to send alerts"
          description: "Alertmanager failed to send {{ $value }} alerts in the last 5 minutes on {{ $labels.instance }}"

  - name: node-exporter-monitoring
    rules:
      # Node Exporter down
      - alert: NodeExporterDown
        expr: up{job="node-exporter"} == 0
        for: 1m
        labels:
          severity: critical
          service: node-exporter
          category: availability
        annotations:
          summary: "Node Exporter is down"
          description: "Node Exporter has been down for more than 1 minute on {{ $labels.instance }}"

  - name: cadvisor-monitoring
    rules:
      # cAdvisor down
      - alert: CAdvisorDown
        expr: up{job="cadvisor"} == 0
        for: 1m
        labels:
          severity: warning
          service: cadvisor
          category: availability
        annotations:
          summary: "cAdvisor is down"
          description: "cAdvisor has been down for more than 1 minute on {{ $labels.instance }}"
