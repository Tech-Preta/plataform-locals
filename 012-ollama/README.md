# ü§ñ Ollama + Open WebUI

Este diret√≥rio cont√©m a configura√ß√£o Docker para executar o Ollama (servidor de modelos LLM) junto com o Open WebUI (interface web) com persist√™ncia de dados.

## üéØ Componentes

### üß† **Ollama** (porta 11434)
- **Fun√ß√£o:** Servidor de modelos de linguagem (LLM) local
- **API:** http://localhost:11434
- **Modelos:** Llama, Mistral, CodeLlama, etc.
- **Dados:** Volume persistente `ollama-data`

### üåê **Open WebUI** (porta 3000)
- **Fun√ß√£o:** Interface web para interagir com modelos
- **UI:** http://localhost:3000
- **Dados:** Volume persistente `open-webui-data`
- **Features:** Chat, hist√≥rico, configura√ß√µes

---

## üöÄ In√≠cio R√°pido

### 1. Subir os servi√ßos
```bash
cd 012-ollama
docker compose up -d
```

### 2. Verificar se est√£o rodando
```bash
docker compose ps
```

### 3. Acessar a interface
- **Open WebUI**: http://localhost:3000
- **API Ollama**: http://localhost:11434

### 4. Criar primeiro usu√°rio
- Acesse http://localhost:3000
- Clique em "Sign Up" 
- Crie sua conta (primeiro usu√°rio ser√° admin)

---

## üì¶ Baixando Modelos

### Via Interface Web (Recomendado)
1. Acesse http://localhost:3000
2. Fa√ßa login
3. Clique no √≠cone de configura√ß√µes (‚öôÔ∏è)
4. V√° em "Models" 
5. Digite o nome do modelo (ex: `llama3.2:3b`)
6. Clique em "Pull Model"

### Via CLI
```bash
# Baixar modelo Llama 3.2 (3B par√¢metros)
docker exec ollama ollama pull llama3.2:3b

# Baixar modelo CodeLlama (para c√≥digo)
docker exec ollama ollama pull codellama:7b

# Baixar modelo Mistral (boa performance)
docker exec ollama ollama pull mistral:7b

# Listar modelos instalados
docker exec ollama ollama list
```

### Via API
```bash
# Baixar modelo via API
curl -X POST http://localhost:11434/api/pull \
  -H "Content-Type: application/json" \
  -d '{"name": "llama3.2:3b"}'
```

---

## üéØ Modelos Recomendados

### üí° **Para Iniciantes (menor uso de recursos)**
| Modelo        | Tamanho | RAM | Descri√ß√£o                        |
| ------------- | ------- | --- | -------------------------------- |
| `llama3.2:1b` | ~1.3GB  | 4GB | Modelo pequeno e r√°pido          |
| `llama3.2:3b` | ~2.0GB  | 6GB | Bom equil√≠brio tamanho/qualidade |
| `qwen2.5:3b`  | ~1.9GB  | 6GB | Modelo chin√™s muito eficiente    |

### üî• **Para Melhor Qualidade (mais recursos)**
| Modelo         | Tamanho | RAM  | Descri√ß√£o                       |
| -------------- | ------- | ---- | ------------------------------- |
| `llama3.2:7b`  | ~4.1GB  | 12GB | Excelente qualidade geral       |
| `mistral:7b`   | ~4.1GB  | 12GB | Muito bom para tarefas variadas |
| `codellama:7b` | ~3.8GB  | 12GB | Especializado em c√≥digo         |

### üöÄ **Para Hardware Potente**
| Modelo         | Tamanho | RAM  | Descri√ß√£o                   |
| -------------- | ------- | ---- | --------------------------- |
| `llama3.1:70b` | ~40GB   | 80GB | Qualidade pr√≥xima ao GPT-4  |
| `qwen2.5:72b`  | ~41GB   | 80GB | Modelo multil√≠ngue avan√ßado |

---

## üñ•Ô∏è Suporte a GPU (NVIDIA)

### Habilitar GPU
1. **Editar docker-compose.yml:**
   ```yaml
   # Descomentar as linhas de GPU no servi√ßo ollama:
   deploy:
     resources:
       reservations:
         devices:
           - driver: nvidia
             count: all
             capabilities: [gpu]
   ```

2. **Verificar NVIDIA Container Toolkit:**
   ```bash
   # Verificar se est√° instalado
   nvidia-smi
   docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi
   ```

3. **Reiniciar servi√ßos:**
   ```bash
   docker compose down
   docker compose up -d
   ```

### Verificar uso de GPU
```bash
# Ver uso de GPU
nvidia-smi

# Logs do Ollama para verificar GPU
docker compose logs ollama | grep -i gpu
```

---

## üîß Configura√ß√£o Avan√ßada

### Vari√°veis de Ambiente - Ollama
```yaml
environment:
  - OLLAMA_HOST=0.0.0.0:11434        # Host/porta do servidor
  - OLLAMA_ORIGINS=*                 # CORS origins permitidas
  - OLLAMA_MODELS=/root/.ollama      # Diret√≥rio dos modelos
  - OLLAMA_KEEP_ALIVE=5m             # Tempo para manter modelo em mem√≥ria
  - OLLAMA_NUM_PARALLEL=1            # Requests paralelos
  - OLLAMA_MAX_LOADED_MODELS=1       # M√°ximo de modelos carregados
```

### Vari√°veis de Ambiente - Open WebUI
```yaml
environment:
  - OLLAMA_BASE_URL=http://ollama:11434  # URL do Ollama
  - WEBUI_SECRET_KEY=your-secret-key     # Chave secreta (MUDE!)
  - WEBUI_JWT_SECRET_KEY=jwt-secret      # JWT secret (MUDE!)
  - ENABLE_SIGNUP=true                   # Permitir cadastro
  - DEFAULT_USER_ROLE=user               # Role padr√£o
  - WEBUI_AUTH=true                      # Autentica√ß√£o obrigat√≥ria
  - MAX_FILE_SIZE=10485760               # Tamanho m√°ximo de arquivo (10MB)
```

---

## üß™ Testando a Instala√ß√£o

### 1. Verificar API do Ollama
```bash
# Verificar se API est√° respondendo
curl http://localhost:11434/api/tags

# Testar gera√ß√£o de texto (ap√≥s baixar um modelo)
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3.2:3b",
    "prompt": "Hello, world!",
    "stream": false
  }'
```

### 2. Verificar Open WebUI
```bash
# Verificar se est√° respondendo
curl -I http://localhost:3000

# Ver logs
docker compose logs open-webui
```

### 3. Teste de Chat
1. Acesse http://localhost:3000
2. Fa√ßa login
3. Inicie um chat
4. Digite: "Ol√°, como voc√™ pode me ajudar?"

---

## üìä Monitoramento e Logs

### Logs dos Servi√ßos
```bash
# Ver logs de ambos os servi√ßos
docker compose logs -f

# Logs apenas do Ollama
docker compose logs -f ollama

# Logs apenas do Open WebUI
docker compose logs -f open-webui
```

### Uso de Recursos
```bash
# Ver uso de recursos
docker stats

# Espa√ßo dos volumes
docker system df -v

# Modelos instalados e tamanhos
docker exec ollama ollama list
```

### Status dos Servi√ßos
```bash
# Status dos containers
docker compose ps

# Verificar health dos servi√ßos
curl -s http://localhost:11434/api/tags | jq .
curl -I http://localhost:3000
```

---

## üîí Seguran√ßa

### üö® **IMPORTANTE: Alterar Chaves Secretas**
```bash
# Gerar chaves seguras
openssl rand -hex 32  # Para WEBUI_SECRET_KEY
openssl rand -hex 32  # Para WEBUI_JWT_SECRET_KEY
```

### üõ°Ô∏è **Configura√ß√µes de Seguran√ßa**
```yaml
environment:
  # Desabilitar cadastro p√∫blico em produ√ß√£o
  - ENABLE_SIGNUP=false
  
  # Definir role padr√£o mais restritivo
  - DEFAULT_USER_ROLE=pending
  
  # Limitar tamanho de arquivos
  - MAX_FILE_SIZE=5242880  # 5MB
```

### üîê **Acesso Restrito (Produ√ß√£o)**
```yaml
# Adicionar proxy reverso com autentica√ß√£o
# Ou restringir portas no firewall
ports:
  - "127.0.0.1:3000:8080"  # Apenas localhost
  - "127.0.0.1:11434:11434"  # Apenas localhost
```

---

## üìÇ Gerenciamento de Dados

### Backup dos Dados
```bash
# Parar servi√ßos
docker compose down

# Backup dos volumes
docker run --rm -v ollama-data:/source -v $(pwd):/backup alpine \
  tar czf /backup/ollama-backup-$(date +%Y%m%d).tar.gz -C /source .

docker run --rm -v open-webui-data:/source -v $(pwd):/backup alpine \
  tar czf /backup/webui-backup-$(date +%Y%m%d).tar.gz -C /source .

# Reiniciar servi√ßos
docker compose up -d
```

### Restaurar Backup
```bash
# Parar servi√ßos
docker compose down

# Restaurar volumes
docker run --rm -v ollama-data:/dest -v $(pwd):/backup alpine \
  tar xzf /backup/ollama-backup-YYYYMMDD.tar.gz -C /dest

docker run --rm -v open-webui-data:/dest -v $(pwd):/backup alpine \
  tar xzf /backup/webui-backup-YYYYMMDD.tar.gz -C /dest

# Reiniciar servi√ßos
docker compose up -d
```

### Limpeza de Espa√ßo
```bash
# Remover modelos n√£o utilizados
docker exec ollama ollama rm MODEL_NAME

# Limpar dados do Docker
docker system prune -f

# Ver espa√ßo usado
du -sh /var/lib/docker/volumes/ollama_*
```

---

## üîß Troubleshooting

### ‚ùå Problemas Comuns

#### 1. **Ollama n√£o inicia**
```bash
# Verificar logs
docker compose logs ollama

# Verificar se porta est√° em uso
sudo netstat -tlnp | grep :11434

# Reiniciar apenas o Ollama
docker compose restart ollama
```

#### 2. **Open WebUI n√£o conecta ao Ollama**
```bash
# Verificar conectividade
docker exec open-webui curl http://ollama:11434/api/tags

# Verificar vari√°vel de ambiente
docker exec open-webui env | grep OLLAMA_BASE_URL
```

#### 3. **Modelo n√£o baixa**
```bash
# Verificar espa√ßo em disco
df -h

# Verificar conectividade
docker exec ollama curl -I https://ollama.com

# Baixar manualmente
docker exec -it ollama ollama pull llama3.2:3b
```

#### 4. **Performance ruim**
```bash
# Verificar recursos
docker stats

# Ajustar configura√ß√µes de mem√≥ria
# Editar OLLAMA_KEEP_ALIVE e OLLAMA_MAX_LOADED_MODELS
```

### üîÑ Reinicializa√ß√£o Limpa
```bash
# Parar tudo
docker compose down

# Remover volumes (CUIDADO: perde dados!)
docker volume rm ollama_ollama-data ollama_open-webui-data

# Recriar e iniciar
docker compose up -d
```

---

## üöÄ Pr√≥ximos Passos

### üîå **Integra√ß√µes Poss√≠veis**
1. **Proxy Reverso** (Traefik/Nginx) para HTTPS
2. **Autentica√ß√£o Externa** (LDAP/OAuth)
3. **Monitoramento** com Prometheus
4. **Load Balancer** para m√∫ltiplas inst√¢ncias

### üì± **Apps Compat√≠veis**
- **Continue.dev** (VS Code extension)
- **Cursor** (AI code editor)
- **LangChain** (development framework)
- **Open Interpreter** (AI assistant)

### üéØ **Casos de Uso**
- **Assistente de c√≥digo** local
- **An√°lise de documentos** privada
- **Chatbot personalizado** para empresa
- **Desenvolvimento AI** sem depend√™ncia de APIs externas

---

## üìö Recursos √öteis

- **Ollama Docs**: https://ollama.com/docs
- **Open WebUI**: https://openwebui.com
- **Model Library**: https://ollama.com/library
- **GPU Support**: https://docs.nvidia.com/datacenter/cloud-native/

---

## ‚úÖ Status: Configura√ß√£o Pronta

üéâ **Seu ambiente Ollama + Open WebUI est√° configurado e pronto para uso!**

**Para iniciar:**
```bash
docker compose up -d
```

**Acesse:** http://localhost:3000
